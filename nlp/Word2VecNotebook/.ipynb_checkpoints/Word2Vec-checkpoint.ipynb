{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# word2vec\n",
    "\n",
    "Ten notatnik ma na celu przedstawienie sposobu tworzenia i wykorzystania reprezentacji werktorowych na przykładzie algorytmu word2vec. W trakcie zadania najpierw stworzymy prostą reprezentację wektorową, a następnie spróbujemy wczytać gotowy model nauczony na dużym korpusie tekstowym.\n",
    "\n",
    "Po wykonaniu tego zadania powinieneś:\n",
    "+ wiedzieć na czym polega word2vec,\n",
    "+ potrafić stworzyć word2vec na własnych danych,\n",
    "+ potrafić wykorzystać word2vec do:\n",
    "\t+ znalezienia podobnych słów,\n",
    "\t+ wyszukiwania słów na zasadzie \"reguły trzech\", \n",
    "\t+ wykrywania niepasujących słów,\n",
    "\t+ do tworzenia wektora cech nadającego się do klasyfikacji,\n",
    "+ wczytać i wykorzystać gotowy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prosty model\n",
    "\n",
    "Najpierw wczytamy odpowiednie biblioteki i stworzymy mały zbiór treningowy na podstawie znanej piosenki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging, re, nltk\n",
    "import pandas as pd\n",
    "\n",
    "RE_SPACES = re.compile(\"\\s+\")\n",
    "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
    "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "song = \"\"\"Gdzie strumyk płynie z wolna,\n",
    "Rozsiewa zioła maj,\n",
    "Stokrotka rosła polna,\n",
    "A nad nią szumiał gaj,\n",
    "Stokrotka rosła polna,\n",
    "A nad nią szumiał gaj,\n",
    "Zielony gaj.\n",
    "\n",
    "W tym gaju tak ponuro,\n",
    "Że aż przeraża mnie,\n",
    "Ptaszęta za wysoko,\n",
    "A mnie samotnej źle,\n",
    "Ptaszęta za wysoko,\n",
    "A mnie samotnej źle,\n",
    "samotnej źle.\n",
    "\n",
    "Wtem harcerz idzie z wolna.\n",
    "„Stokrotko, witam cię,\n",
    "Twój urok mnie zachwyca,\n",
    "Czy chcesz być mą, czy nie?”\n",
    "\"Twój urok mnie zachwyca,\n",
    "Czy chcesz być mą, czy nie?\n",
    "Czy nie, czy nie?\n",
    "\n",
    "Stokrotka się zgodziła\n",
    "I poszli w ciemny las,\n",
    "A harcerz taki gapa\n",
    "że aż w pokrzywy wlazł,\n",
    "A harcerz taki gapa\n",
    "że aż w pokrzywy wlazł,\n",
    "w pokrzywy wlazł.\n",
    "\n",
    "A ona, ona, ona,\n",
    "Cóż biedna robić ma,\n",
    "Nad gapą pochylona\n",
    "I śmieje się: ha, ha,\n",
    "Nad gapą pochylona\n",
    "I śmieje: się ha, ha,\n",
    "ha, ha, ha, ha.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 1: Podziel piosenkę na wersy, a wersy tokenizuj spacjami. W efekcie powinieneś stworzyć listę list i przypisać ją do zmiennej `sentences`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Gdzie', 'strumyk', 'płynie', 'z', 'wolna', ','], ['Rozsiewa', 'zioła', 'maj', ','], ['Stokrotka', 'rosła', 'polna', ','], ['A', 'nad', 'nią', 'szumiał', 'gaj', ','], ['Stokrotka', 'rosła', 'polna', ','], ['A', 'nad', 'nią', 'szumiał', 'gaj', ','], ['Zielony', 'gaj', '.'], [], ['W', 'tym', 'gaju', 'tak', 'ponuro', ','], ['Że', 'aż', 'przeraża', 'mnie', ','], ['Ptaszęta', 'za', 'wysoko', ','], ['A', 'mnie', 'samotnej', 'źle', ','], ['Ptaszęta', 'za', 'wysoko', ','], ['A', 'mnie', 'samotnej', 'źle', ','], ['samotnej', 'źle', '.'], [], ['Wtem', 'harcerz', 'idzie', 'z', 'wolna', '.'], ['„', 'Stokrotko', ',', 'witam', 'cię', ','], ['Twój', 'urok', 'mnie', 'zachwyca', ','], ['Czy', 'chcesz', 'być', 'mą', ',', 'czy', 'nie', '?', '”'], ['``', 'Twój', 'urok', 'mnie', 'zachwyca', ','], ['Czy', 'chcesz', 'być', 'mą', ',', 'czy', 'nie', '?'], ['Czy', 'nie', ',', 'czy', 'nie', '?'], [], ['Stokrotka', 'się', 'zgodziła'], ['I', 'poszli', 'w', 'ciemny', 'las', ','], ['A', 'harcerz', 'taki', 'gapa'], ['że', 'aż', 'w', 'pokrzywy', 'wlazł', ','], ['A', 'harcerz', 'taki', 'gapa'], ['że', 'aż', 'w', 'pokrzywy', 'wlazł', ','], ['w', 'pokrzywy', 'wlazł', '.'], [], ['A', 'ona', ',', 'ona', ',', 'ona', ','], ['Cóż', 'biedna', 'robić', 'ma', ','], ['Nad', 'gapą', 'pochylona'], ['I', 'śmieje', 'się', ':', 'ha', ',', 'ha', ','], ['Nad', 'gapą', 'pochylona'], ['I', 'śmieje', ':', 'się', 'ha', ',', 'ha', ','], ['ha', ',', 'ha', ',', 'ha', ',', 'ha', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lines = song.splitlines()\n",
    "\n",
    "sentences = [word_tokenize(line) for line in lines]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając tekst podzielony na zdania a zdania na tokeny, możemy nauczyć model word2vec.\n",
    "\n",
    "**Zad. 2: Naucz model word2vec. [Sprawdź](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) za co odpowiedzialne są parametry `min_count` i `iter`. Jakie inne parametry mogą być przydatne?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 23:23:25,998 : INFO : collecting all words and their counts\n",
      "2023-12-21 23:23:25,999 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-12-21 23:23:26,000 : INFO : collected 75 word types from a corpus of 186 raw words and 39 sentences\n",
      "2023-12-21 23:23:26,000 : INFO : Creating a fresh vocabulary\n",
      "2023-12-21 23:23:26,001 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 75 unique words (100.00% of original 75, drops 0)', 'datetime': '2023-12-21T23:23:26.001312', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-21 23:23:26,001 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 186 word corpus (100.00% of original 186, drops 0)', 'datetime': '2023-12-21T23:23:26.001312', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-21 23:23:26,002 : INFO : deleting the raw counts dictionary of 75 items\n",
      "2023-12-21 23:23:26,003 : INFO : sample=0.001 downsamples 75 most-common words\n",
      "2023-12-21 23:23:26,003 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 60.476489292396835 word corpus (32.5%% of prior 186)', 'datetime': '2023-12-21T23:23:26.003311', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-21 23:23:26,004 : INFO : estimated required memory for 75 words and 100 dimensions: 97500 bytes\n",
      "2023-12-21 23:23:26,006 : INFO : resetting layer weights\n",
      "2023-12-21 23:23:26,006 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-12-21T23:23:26.006310', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2023-12-21 23:23:26,007 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 75 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-12-21T23:23:26.007311', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2023-12-21 23:23:26,009 : INFO : EPOCH 0: training on 186 raw words (57 effective words) took 0.0s, 137283 effective words/s\n",
      "2023-12-21 23:23:26,011 : INFO : EPOCH 1: training on 186 raw words (60 effective words) took 0.0s, 135777 effective words/s\n",
      "2023-12-21 23:23:26,013 : INFO : EPOCH 2: training on 186 raw words (67 effective words) took 0.0s, 147059 effective words/s\n",
      "2023-12-21 23:23:26,015 : INFO : EPOCH 3: training on 186 raw words (57 effective words) took 0.0s, 150794 effective words/s\n",
      "2023-12-21 23:23:26,016 : INFO : EPOCH 4: training on 186 raw words (68 effective words) took 0.0s, 170726 effective words/s\n",
      "2023-12-21 23:23:26,017 : INFO : Word2Vec lifecycle event {'msg': 'training on 930 raw words (309 effective words) took 0.0s, 32002 effective words/s', 'datetime': '2023-12-21T23:23:26.017312', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2023-12-21 23:23:26,017 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=75, vector_size=100, alpha=0.025>', 'datetime': '2023-12-21T23:23:26.017312', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=75, vector_size=100, alpha=0.025>\n",
      "{',': 0, 'ha': 1, 'A': 2, '.': 3, 'mnie': 4, 'w': 5, 'nie': 6, 'aż': 7, 'harcerz': 8, 'czy': 9, '?': 10, 'Stokrotka': 11, 'się': 12, 'gaj': 13, 'I': 14, 'Czy': 15, 'źle': 16, 'samotnej': 17, 'pokrzywy': 18, 'wlazł': 19, 'ona': 20, 'szumiał': 21, 'nią': 22, 'nad': 23, 'zachwyca': 24, ':': 25, 'gapą': 26, 'wysoko': 27, 'polna': 28, 'Twój': 29, 'urok': 30, 'chcesz': 31, 'pochylona': 32, 'z': 33, 'być': 34, 'mą': 35, 'Ptaszęta': 36, 'rosła': 37, 'Nad': 38, 'śmieje': 39, 'taki': 40, 'gapa': 41, 'że': 42, 'wolna': 43, 'za': 44, 'Zielony': 45, 'płynie': 46, 'tym': 47, 'maj': 48, 'zioła': 49, 'Rozsiewa': 50, 'strumyk': 51, 'W': 52, 'Stokrotko': 53, 'gaju': 54, '``': 55, 'ma': 56, 'robić': 57, 'biedna': 58, 'Cóż': 59, 'las': 60, 'ciemny': 61, 'poszli': 62, 'zgodziła': 63, '”': 64, 'tak': 65, 'cię': 66, 'witam': 67, '„': 68, 'idzie': 69, 'Wtem': 70, 'przeraża': 71, 'Że': 72, 'ponuro': 73, 'Gdzie': 74}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'harcerz'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, epochs=5, min_count=1)\n",
    "print(model)\n",
    "print(model.wv.key_to_index)\n",
    "\n",
    "model.wv.doesnt_match(\"las harcerz gaj zioła\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model jest niezwykle mały i niezbyt praktyczny, ale pozwolił pokazać podstawę uczenia word2vec. Przy większych korpusach tekstowych wczytywanie do pamięci wielkich tablic nie byłoby najlepszym pomysłem. Na szczęście implementacja word2vec w gensim potrafi przetwarzać dane przyrostowo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przetwarzanie strumieniowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast wczytywać wszystkie dokumenty naraz można robić to partiami, bo sieci neuronowe (w tym word2vec) potrafią douczać się przyrostowo. Do douczania przyrostowego świetnie nada się pythonowy iterator lub generator. Jeśli nie kojarzysz na czym polega działanie iteratorów i generatorów, zobacz jak [wyjaśnia to Radim Rehurek](https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/).\n",
    "\n",
    "Zasymulujmy zdania/wersy/tweety przechowywane w osobnych plikach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open, os\n",
    "\n",
    "if not os.path.exists('./data/'):\n",
    "    os.makedirs('./data/')\n",
    "\n",
    "filenames = ['./data/f' + str(i) +'.txt' for i in range(39)]\n",
    "\n",
    "if sentences is not None:\n",
    "    for i, fname in enumerate(filenames):\n",
    "        with smart_open.smart_open(fname, 'w', encoding='utf-8') as fout:\n",
    "            for line in sentences[i]:\n",
    "                fout.write(line + ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 3: Mając powyższy zbiór dokumentów tekstowych, stwórz metodę która będzie \"leniwie\" iterowała przez zasymulowany zbiór danych. Podczas iterowania usuń znaki interpunkcyjne i zmień wszystkie litery na małe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 23:44:42,275 : INFO : collecting all words and their counts\n",
      "2023-12-21 23:44:42,276 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-12-21 23:44:42,284 : INFO : collected 64 word types from a corpus of 140 raw words and 35 sentences\n",
      "2023-12-21 23:44:42,284 : INFO : Creating a fresh vocabulary\n",
      "2023-12-21 23:44:42,285 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 64 unique words (100.00% of original 64, drops 0)', 'datetime': '2023-12-21T23:44:42.285959', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-21 23:44:42,285 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 140 word corpus (100.00% of original 140, drops 0)', 'datetime': '2023-12-21T23:44:42.285959', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-21 23:44:42,286 : INFO : deleting the raw counts dictionary of 64 items\n",
      "2023-12-21 23:44:42,286 : INFO : sample=0.001 downsamples 64 most-common words\n",
      "2023-12-21 23:44:42,286 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 42.80878571413852 word corpus (30.6%% of prior 140)', 'datetime': '2023-12-21T23:44:42.286958', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-21 23:44:42,287 : INFO : estimated required memory for 64 words and 100 dimensions: 83200 bytes\n",
      "2023-12-21 23:44:42,288 : INFO : resetting layer weights\n",
      "2023-12-21 23:44:42,289 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-12-21T23:44:42.289958', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2023-12-21 23:44:42,289 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 64 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-12-21T23:44:42.289958', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2023-12-21 23:44:42,298 : INFO : EPOCH 0: training on 140 raw words (46 effective words) took 0.0s, 6735 effective words/s\n",
      "2023-12-21 23:44:42,306 : INFO : EPOCH 1: training on 140 raw words (45 effective words) took 0.0s, 6924 effective words/s\n",
      "2023-12-21 23:44:42,314 : INFO : EPOCH 2: training on 140 raw words (39 effective words) took 0.0s, 5664 effective words/s\n",
      "2023-12-21 23:44:42,323 : INFO : EPOCH 3: training on 140 raw words (44 effective words) took 0.0s, 6604 effective words/s\n",
      "2023-12-21 23:44:42,332 : INFO : EPOCH 4: training on 140 raw words (35 effective words) took 0.0s, 5066 effective words/s\n",
      "2023-12-21 23:44:42,340 : INFO : EPOCH 5: training on 140 raw words (38 effective words) took 0.0s, 5462 effective words/s\n",
      "2023-12-21 23:44:42,349 : INFO : EPOCH 6: training on 140 raw words (41 effective words) took 0.0s, 6015 effective words/s\n",
      "2023-12-21 23:44:42,357 : INFO : EPOCH 7: training on 140 raw words (38 effective words) took 0.0s, 5557 effective words/s\n",
      "2023-12-21 23:44:42,366 : INFO : EPOCH 8: training on 140 raw words (45 effective words) took 0.0s, 6893 effective words/s\n",
      "2023-12-21 23:44:42,374 : INFO : EPOCH 9: training on 140 raw words (50 effective words) took 0.0s, 7186 effective words/s\n",
      "2023-12-21 23:44:42,375 : INFO : Word2Vec lifecycle event {'msg': 'training on 1400 raw words (421 effective words) took 0.1s, 4949 effective words/s', 'datetime': '2023-12-21T23:44:42.375959', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2023-12-21 23:44:42,375 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=64, vector_size=100, alpha=0.025>', 'datetime': '2023-12-21T23:44:42.375959', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=64, vector_size=100, alpha=0.025>\n",
      "{'ha': 0, 'a': 1, 'czy': 2, 'mnie': 3, 'w': 4, 'nad': 5, 'nie': 6, 'gaj': 7, 'i': 8, 'że': 9, 'aż': 10, 'pokrzywy': 11, 'wlazł': 12, 'się': 13, 'samotnej': 14, 'harcerz': 15, 'ona': 16, 'stokrotka': 17, 'źle': 18, 'mą': 19, 'być': 20, 'chcesz': 21, 'zachwyca': 22, 'urok': 23, 'twój': 24, 'rosła': 25, 'polna': 26, 'wysoko': 27, 'za': 28, 'z': 29, 'wolna': 30, 'śmieje': 31, 'pochylona': 32, 'gapą': 33, 'gapa': 34, 'ptaszęta': 35, 'szumiał': 36, 'nią': 37, 'taki': 38, 'zioła': 39, 'maj': 40, 'idzie': 41, 'płynie': 42, 'strumyk': 43, 'wtem': 44, 'rozsiewa': 45, 'przeraża': 46, 'stokrotko': 47, 'biedna': 48, 'tak': 49, 'gaju': 50, 'tym': 51, 'zielony': 52, 'ma': 53, 'robić': 54, 'cóż': 55, 'witam': 56, 'las': 57, 'ciemny': 58, 'poszli': 59, 'zgodziła': 60, 'ponuro': 61, 'cię': 62, 'gdzie': 63}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'zioła'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        self.translator = str.maketrans('', '', string.punctuation + '„“”') \n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            if fname.endswith('.txt'):\n",
    "                for line in open(os.path.join(self.dirname, fname), encoding='utf-8'):\n",
    "                    clean_line = line.translate(self.translator)\n",
    "                    clean_line = clean_line.lower()\n",
    "                    yield word_tokenize(clean_line)\n",
    "\n",
    "# Do odkomentowania:\n",
    "sentences = MySentences('./data/')\n",
    "model = gensim.models.Word2Vec(sentences, epochs=10, min_count=1)\n",
    "print(model)\n",
    "print(model.wv.key_to_index)\n",
    "\n",
    "model.wv.doesnt_match(\"las harcerz gaj zioła\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trochę więcej danych i przykłady zastosowań\n",
    "\n",
    "Jak wspomniano wcześniej powyższa piosenka jest zbyt krótka by stworzyć przekonujący model podobieństwa między słowami. Przejdziemy teraz na język angielski i wykorzystamy korpus dołączony do biblioteki `gensim`. Ten korpus nie jest jeszcze duży, więc wyniki nie będą rewelacyjne. Potrzeba > 500 tys. słów, żeby oczekiwać rozsądnych wyników dla ogólnych zapytań, ale przy specjalistycznych zastosowaniach korpusy niekoniecznie muszą być takie duże.\n",
    "\n",
    "**Zad. 4: Korzystając ze zdobytej wiedzy na temat iteratorów, uzupełnij poniższy kod.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
    "lee_train_file = test_data_dir + 'lee_background.cor'\n",
    "\n",
    "class MyText(object):\n",
    "    def __init__(self):\n",
    "        self.translator = str.maketrans('', '', string.punctuation + '„“”') \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for line in open(lee_train_file):\n",
    "            # Załóż, że każda linia to dokument, zmień litery na małe,\n",
    "            # usuń podstawowe znaki interpunkcyjne i podziel według białych znaków\n",
    "            clean_line = line.translate(self.translator)\n",
    "            clean_line = clean_line.lower()\n",
    "            yield word_tokenize(clean_line)\n",
    "\n",
    "sentences = MyText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 5: Naucz model word2vec o rozmiarze 200, przez 100 epok, usuwając słowa występującerzadziej niż 5 razy. Wynik przypisz do zmiennej `model`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 23:59:30,380 : INFO : collecting all words and their counts\n",
      "2023-12-21 23:59:30,382 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-12-21 23:59:30,524 : INFO : collected 7584 word types from a corpus of 59857 raw words and 300 sentences\n",
      "2023-12-21 23:59:30,525 : INFO : Creating a fresh vocabulary\n",
      "2023-12-21 23:59:30,530 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1789 unique words (23.59% of original 7584, drops 5795)', 'datetime': '2023-12-21T23:59:30.530976', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-21 23:59:30,531 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 50263 word corpus (83.97% of original 59857, drops 9594)', 'datetime': '2023-12-21T23:59:30.531979', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-21 23:59:30,539 : INFO : deleting the raw counts dictionary of 7584 items\n",
      "2023-12-21 23:59:30,540 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2023-12-21 23:59:30,541 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 36158.92205282653 word corpus (71.9%% of prior 50263)', 'datetime': '2023-12-21T23:59:30.541983', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-21 23:59:30,552 : INFO : estimated required memory for 1789 words and 200 dimensions: 3756900 bytes\n",
      "2023-12-21 23:59:30,553 : INFO : resetting layer weights\n",
      "2023-12-21 23:59:30,554 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-12-21T23:59:30.554980', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2023-12-21 23:59:30,555 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1789 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-12-21T23:59:30.555980', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2023-12-21 23:59:30,725 : INFO : EPOCH 0: training on 59857 raw words (36213 effective words) took 0.2s, 215578 effective words/s\n",
      "2023-12-21 23:59:30,891 : INFO : EPOCH 1: training on 59857 raw words (36068 effective words) took 0.2s, 219905 effective words/s\n",
      "2023-12-21 23:59:31,058 : INFO : EPOCH 2: training on 59857 raw words (36136 effective words) took 0.2s, 218990 effective words/s\n",
      "2023-12-21 23:59:31,224 : INFO : EPOCH 3: training on 59857 raw words (36122 effective words) took 0.2s, 219533 effective words/s\n",
      "2023-12-21 23:59:31,388 : INFO : EPOCH 4: training on 59857 raw words (36165 effective words) took 0.2s, 222860 effective words/s\n",
      "2023-12-21 23:59:31,557 : INFO : EPOCH 5: training on 59857 raw words (36145 effective words) took 0.2s, 215792 effective words/s\n",
      "2023-12-21 23:59:31,726 : INFO : EPOCH 6: training on 59857 raw words (36124 effective words) took 0.2s, 216191 effective words/s\n",
      "2023-12-21 23:59:31,894 : INFO : EPOCH 7: training on 59857 raw words (36119 effective words) took 0.2s, 218110 effective words/s\n",
      "2023-12-21 23:59:32,058 : INFO : EPOCH 8: training on 59857 raw words (36091 effective words) took 0.2s, 222074 effective words/s\n",
      "2023-12-21 23:59:32,224 : INFO : EPOCH 9: training on 59857 raw words (36081 effective words) took 0.2s, 219616 effective words/s\n",
      "2023-12-21 23:59:32,391 : INFO : EPOCH 10: training on 59857 raw words (36232 effective words) took 0.2s, 219424 effective words/s\n",
      "2023-12-21 23:59:32,559 : INFO : EPOCH 11: training on 59857 raw words (36191 effective words) took 0.2s, 217169 effective words/s\n",
      "2023-12-21 23:59:32,743 : INFO : EPOCH 12: training on 59857 raw words (36225 effective words) took 0.2s, 199003 effective words/s\n",
      "2023-12-21 23:59:32,912 : INFO : EPOCH 13: training on 59857 raw words (36195 effective words) took 0.2s, 216595 effective words/s\n",
      "2023-12-21 23:59:33,087 : INFO : EPOCH 14: training on 59857 raw words (36235 effective words) took 0.2s, 208988 effective words/s\n",
      "2023-12-21 23:59:33,263 : INFO : EPOCH 15: training on 59857 raw words (36194 effective words) took 0.2s, 207839 effective words/s\n",
      "2023-12-21 23:59:33,451 : INFO : EPOCH 16: training on 59857 raw words (36182 effective words) took 0.2s, 194622 effective words/s\n",
      "2023-12-21 23:59:33,640 : INFO : EPOCH 17: training on 59857 raw words (36152 effective words) took 0.2s, 193447 effective words/s\n",
      "2023-12-21 23:59:33,800 : INFO : EPOCH 18: training on 59857 raw words (36244 effective words) took 0.2s, 230030 effective words/s\n",
      "2023-12-21 23:59:33,969 : INFO : EPOCH 19: training on 59857 raw words (36150 effective words) took 0.2s, 215180 effective words/s\n",
      "2023-12-21 23:59:34,144 : INFO : EPOCH 20: training on 59857 raw words (36116 effective words) took 0.2s, 209700 effective words/s\n",
      "2023-12-21 23:59:34,315 : INFO : EPOCH 21: training on 59857 raw words (36163 effective words) took 0.2s, 213221 effective words/s\n",
      "2023-12-21 23:59:34,479 : INFO : EPOCH 22: training on 59857 raw words (36258 effective words) took 0.2s, 222145 effective words/s\n",
      "2023-12-21 23:59:34,650 : INFO : EPOCH 23: training on 59857 raw words (36165 effective words) took 0.2s, 213621 effective words/s\n",
      "2023-12-21 23:59:34,818 : INFO : EPOCH 24: training on 59857 raw words (36293 effective words) took 0.2s, 219191 effective words/s\n",
      "2023-12-21 23:59:34,988 : INFO : EPOCH 25: training on 59857 raw words (36110 effective words) took 0.2s, 214884 effective words/s\n",
      "2023-12-21 23:59:35,153 : INFO : EPOCH 26: training on 59857 raw words (36155 effective words) took 0.2s, 222595 effective words/s\n",
      "2023-12-21 23:59:35,321 : INFO : EPOCH 27: training on 59857 raw words (36209 effective words) took 0.2s, 217816 effective words/s\n",
      "2023-12-21 23:59:35,490 : INFO : EPOCH 28: training on 59857 raw words (36107 effective words) took 0.2s, 216299 effective words/s\n",
      "2023-12-21 23:59:35,655 : INFO : EPOCH 29: training on 59857 raw words (36233 effective words) took 0.2s, 222177 effective words/s\n",
      "2023-12-21 23:59:35,815 : INFO : EPOCH 30: training on 59857 raw words (36173 effective words) took 0.2s, 227820 effective words/s\n",
      "2023-12-21 23:59:35,975 : INFO : EPOCH 31: training on 59857 raw words (36164 effective words) took 0.2s, 228426 effective words/s\n",
      "2023-12-21 23:59:36,148 : INFO : EPOCH 32: training on 59857 raw words (36156 effective words) took 0.2s, 211692 effective words/s\n",
      "2023-12-21 23:59:36,311 : INFO : EPOCH 33: training on 59857 raw words (36053 effective words) took 0.2s, 223869 effective words/s\n",
      "2023-12-21 23:59:36,482 : INFO : EPOCH 34: training on 59857 raw words (36062 effective words) took 0.2s, 213166 effective words/s\n",
      "2023-12-21 23:59:36,673 : INFO : EPOCH 35: training on 59857 raw words (36249 effective words) took 0.2s, 191812 effective words/s\n",
      "2023-12-21 23:59:36,842 : INFO : EPOCH 36: training on 59857 raw words (36143 effective words) took 0.2s, 215881 effective words/s\n",
      "2023-12-21 23:59:37,003 : INFO : EPOCH 37: training on 59857 raw words (36161 effective words) took 0.2s, 227686 effective words/s\n",
      "2023-12-21 23:59:37,164 : INFO : EPOCH 38: training on 59857 raw words (36268 effective words) took 0.2s, 227874 effective words/s\n",
      "2023-12-21 23:59:37,325 : INFO : EPOCH 39: training on 59857 raw words (36215 effective words) took 0.2s, 226210 effective words/s\n",
      "2023-12-21 23:59:37,485 : INFO : EPOCH 40: training on 59857 raw words (36079 effective words) took 0.2s, 227272 effective words/s\n",
      "2023-12-21 23:59:37,645 : INFO : EPOCH 41: training on 59857 raw words (36171 effective words) took 0.2s, 228579 effective words/s\n",
      "2023-12-21 23:59:37,809 : INFO : EPOCH 42: training on 59857 raw words (36138 effective words) took 0.2s, 222866 effective words/s\n",
      "2023-12-21 23:59:37,981 : INFO : EPOCH 43: training on 59857 raw words (36045 effective words) took 0.2s, 212579 effective words/s\n",
      "2023-12-21 23:59:38,148 : INFO : EPOCH 44: training on 59857 raw words (36104 effective words) took 0.2s, 217223 effective words/s\n",
      "2023-12-21 23:59:38,316 : INFO : EPOCH 45: training on 59857 raw words (36175 effective words) took 0.2s, 217794 effective words/s\n",
      "2023-12-21 23:59:38,480 : INFO : EPOCH 46: training on 59857 raw words (36169 effective words) took 0.2s, 223959 effective words/s\n",
      "2023-12-21 23:59:38,647 : INFO : EPOCH 47: training on 59857 raw words (36290 effective words) took 0.2s, 218596 effective words/s\n",
      "2023-12-21 23:59:38,818 : INFO : EPOCH 48: training on 59857 raw words (36069 effective words) took 0.2s, 214037 effective words/s\n",
      "2023-12-21 23:59:38,985 : INFO : EPOCH 49: training on 59857 raw words (36231 effective words) took 0.2s, 219005 effective words/s\n",
      "2023-12-21 23:59:39,145 : INFO : EPOCH 50: training on 59857 raw words (36140 effective words) took 0.2s, 228770 effective words/s\n",
      "2023-12-21 23:59:39,307 : INFO : EPOCH 51: training on 59857 raw words (36245 effective words) took 0.2s, 226502 effective words/s\n",
      "2023-12-21 23:59:39,471 : INFO : EPOCH 52: training on 59857 raw words (36121 effective words) took 0.2s, 221623 effective words/s\n",
      "2023-12-21 23:59:39,632 : INFO : EPOCH 53: training on 59857 raw words (36255 effective words) took 0.2s, 228192 effective words/s\n",
      "2023-12-21 23:59:39,793 : INFO : EPOCH 54: training on 59857 raw words (36049 effective words) took 0.2s, 227206 effective words/s\n",
      "2023-12-21 23:59:39,954 : INFO : EPOCH 55: training on 59857 raw words (36016 effective words) took 0.2s, 225004 effective words/s\n",
      "2023-12-21 23:59:40,114 : INFO : EPOCH 56: training on 59857 raw words (36148 effective words) took 0.2s, 228617 effective words/s\n",
      "2023-12-21 23:59:40,275 : INFO : EPOCH 57: training on 59857 raw words (36260 effective words) took 0.2s, 227531 effective words/s\n",
      "2023-12-21 23:59:40,439 : INFO : EPOCH 58: training on 59857 raw words (36177 effective words) took 0.2s, 223140 effective words/s\n",
      "2023-12-21 23:59:40,600 : INFO : EPOCH 59: training on 59857 raw words (36262 effective words) took 0.2s, 227997 effective words/s\n",
      "2023-12-21 23:59:40,761 : INFO : EPOCH 60: training on 59857 raw words (36173 effective words) took 0.2s, 226476 effective words/s\n",
      "2023-12-21 23:59:40,923 : INFO : EPOCH 61: training on 59857 raw words (36112 effective words) took 0.2s, 225102 effective words/s\n",
      "2023-12-21 23:59:41,083 : INFO : EPOCH 62: training on 59857 raw words (36210 effective words) took 0.2s, 228700 effective words/s\n",
      "2023-12-21 23:59:41,245 : INFO : EPOCH 63: training on 59857 raw words (36214 effective words) took 0.2s, 226517 effective words/s\n",
      "2023-12-21 23:59:41,406 : INFO : EPOCH 64: training on 59857 raw words (36177 effective words) took 0.2s, 227625 effective words/s\n",
      "2023-12-21 23:59:41,566 : INFO : EPOCH 65: training on 59857 raw words (36062 effective words) took 0.2s, 227320 effective words/s\n",
      "2023-12-21 23:59:41,730 : INFO : EPOCH 66: training on 59857 raw words (36156 effective words) took 0.2s, 222877 effective words/s\n",
      "2023-12-21 23:59:41,894 : INFO : EPOCH 67: training on 59857 raw words (36202 effective words) took 0.2s, 223129 effective words/s\n",
      "2023-12-21 23:59:42,060 : INFO : EPOCH 68: training on 59857 raw words (36104 effective words) took 0.2s, 220548 effective words/s\n",
      "2023-12-21 23:59:42,222 : INFO : EPOCH 69: training on 59857 raw words (36136 effective words) took 0.2s, 225277 effective words/s\n",
      "2023-12-21 23:59:42,381 : INFO : EPOCH 70: training on 59857 raw words (36244 effective words) took 0.2s, 229963 effective words/s\n",
      "2023-12-21 23:59:42,542 : INFO : EPOCH 71: training on 59857 raw words (36062 effective words) took 0.2s, 225900 effective words/s\n",
      "2023-12-21 23:59:42,703 : INFO : EPOCH 72: training on 59857 raw words (36198 effective words) took 0.2s, 227492 effective words/s\n",
      "2023-12-21 23:59:42,868 : INFO : EPOCH 73: training on 59857 raw words (36151 effective words) took 0.2s, 222047 effective words/s\n",
      "2023-12-21 23:59:43,028 : INFO : EPOCH 74: training on 59857 raw words (36100 effective words) took 0.2s, 226987 effective words/s\n",
      "2023-12-21 23:59:43,189 : INFO : EPOCH 75: training on 59857 raw words (36211 effective words) took 0.2s, 227951 effective words/s\n",
      "2023-12-21 23:59:43,350 : INFO : EPOCH 76: training on 59857 raw words (36225 effective words) took 0.2s, 227988 effective words/s\n",
      "2023-12-21 23:59:43,509 : INFO : EPOCH 77: training on 59857 raw words (36143 effective words) took 0.2s, 229020 effective words/s\n",
      "2023-12-21 23:59:43,670 : INFO : EPOCH 78: training on 59857 raw words (36191 effective words) took 0.2s, 227133 effective words/s\n",
      "2023-12-21 23:59:43,829 : INFO : EPOCH 79: training on 59857 raw words (36080 effective words) took 0.2s, 229123 effective words/s\n",
      "2023-12-21 23:59:43,990 : INFO : EPOCH 80: training on 59857 raw words (36147 effective words) took 0.2s, 227920 effective words/s\n",
      "2023-12-21 23:59:44,148 : INFO : EPOCH 81: training on 59857 raw words (36086 effective words) took 0.2s, 229650 effective words/s\n",
      "2023-12-21 23:59:44,309 : INFO : EPOCH 82: training on 59857 raw words (36172 effective words) took 0.2s, 227473 effective words/s\n",
      "2023-12-21 23:59:44,472 : INFO : EPOCH 83: training on 59857 raw words (36133 effective words) took 0.2s, 223836 effective words/s\n",
      "2023-12-21 23:59:44,632 : INFO : EPOCH 84: training on 59857 raw words (36171 effective words) took 0.2s, 229659 effective words/s\n",
      "2023-12-21 23:59:44,792 : INFO : EPOCH 85: training on 59857 raw words (36145 effective words) took 0.2s, 228395 effective words/s\n",
      "2023-12-21 23:59:44,954 : INFO : EPOCH 86: training on 59857 raw words (36289 effective words) took 0.2s, 226461 effective words/s\n",
      "2023-12-21 23:59:45,112 : INFO : EPOCH 87: training on 59857 raw words (36118 effective words) took 0.2s, 230314 effective words/s\n",
      "2023-12-21 23:59:45,273 : INFO : EPOCH 88: training on 59857 raw words (36145 effective words) took 0.2s, 226740 effective words/s\n",
      "2023-12-21 23:59:45,433 : INFO : EPOCH 89: training on 59857 raw words (36193 effective words) took 0.2s, 228304 effective words/s\n",
      "2023-12-21 23:59:45,595 : INFO : EPOCH 90: training on 59857 raw words (36226 effective words) took 0.2s, 226863 effective words/s\n",
      "2023-12-21 23:59:45,754 : INFO : EPOCH 91: training on 59857 raw words (36170 effective words) took 0.2s, 229473 effective words/s\n",
      "2023-12-21 23:59:45,914 : INFO : EPOCH 92: training on 59857 raw words (36075 effective words) took 0.2s, 227586 effective words/s\n",
      "2023-12-21 23:59:46,075 : INFO : EPOCH 93: training on 59857 raw words (36337 effective words) took 0.2s, 228587 effective words/s\n",
      "2023-12-21 23:59:46,236 : INFO : EPOCH 94: training on 59857 raw words (36249 effective words) took 0.2s, 227410 effective words/s\n",
      "2023-12-21 23:59:46,400 : INFO : EPOCH 95: training on 59857 raw words (36344 effective words) took 0.2s, 223929 effective words/s\n",
      "2023-12-21 23:59:46,562 : INFO : EPOCH 96: training on 59857 raw words (36094 effective words) took 0.2s, 225583 effective words/s\n",
      "2023-12-21 23:59:46,724 : INFO : EPOCH 97: training on 59857 raw words (36260 effective words) took 0.2s, 226983 effective words/s\n",
      "2023-12-21 23:59:46,885 : INFO : EPOCH 98: training on 59857 raw words (36189 effective words) took 0.2s, 226280 effective words/s\n",
      "2023-12-21 23:59:47,047 : INFO : EPOCH 99: training on 59857 raw words (36230 effective words) took 0.2s, 225441 effective words/s\n",
      "2023-12-21 23:59:47,048 : INFO : Word2Vec lifecycle event {'msg': 'training on 5985700 raw words (3616785 effective words) took 16.5s, 219300 effective words/s', 'datetime': '2023-12-21T23:59:47.048807', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2023-12-21 23:59:47,048 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1789, vector_size=200, alpha=0.025>', 'datetime': '2023-12-21T23:59:47.048807', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=1789, vector_size=200, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, epochs=100, min_count=5, vector_size=200)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 6: Odkomentuj poniższe linie i zobacz jak można wykorzystać uzyskany model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rights', 0.47615838050842285),\n",
       " ('related', 0.4077345132827759),\n",
       " ('attacks', 0.3697214126586914),\n",
       " ('connection', 0.3676607012748718),\n",
       " ('stage', 0.3633438050746918)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['human', 'crime'], negative=['party'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 00:00:29,144 : WARNING : vectors for words {'lunch', 'cat', 'input'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"input is lunch he sentence cat\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036004007\n",
      "0.31288484\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('human', 'tree'))\n",
    "print(model.wv.similarity('crime', 'murder'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uwagi dodatkowe:**\n",
    "+ uczenie modelu można zrównoleglić, ale trzeba doinstalować [Cythona](http://cython.org/)\n",
    "+ wytrenowany model można łatwo zapisać do pliku za pomocą: `model.save(path)`\n",
    "+ równie łatwo można go później wczytać: `model = gensim.models.Word2Vec.load(path)`\n",
    "+ ponieważ uczenie jest przyrostowe, można łatwo rozszerzyć istniejący słownik i douczyć model na nowych zdaniach:\n",
    "```\n",
    "model = gensim.models.Word2Vec.load(path)\n",
    "more_sentences = [['Advanced', 'users', 'can', 'load', 'a', 'model', 'and', 'continue', \n",
    "                  'training', 'it', 'with', 'more', 'sentences']]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystanie gotowego modelu do klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Póki co sami trenowaliśmy word2vec i to na niedużych zbiorach danych. Na szczęście są już gotowe modele (przynajmniej dla języka angielskiego) nauczone na miliardach dokumentów i zawierające miliony słów. Przydatna lista takich modeli (wraz z kodem tworzącym usługę sieciową wykorzystującą model...) pod adresem: https://github.com/3Top/word2vec-api.\n",
    "\n",
    "**Zad. 7: Pobierz korpus Google News i zapisz pobrany plik do folderu data. Następnie wykonaj poniższy kod. Ta operacja zajmie jakieś 3-4 minuty i zużyje ok. 4 GB RAMU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 00:09:35,208 : INFO : loading projection weights from data/GoogleNews-vectors-negative300.bin.gz\n",
      "2023-12-22 00:10:08,930 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from data/GoogleNews-vectors-negative300.bin.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-12-22T00:10:08.930563', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n",
      "<timed exec>:2: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "2023-12-22 00:10:11,004 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 36.3 s\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zad. 8: Zobacz jak działa model nauczony na tak dużym korpusie.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192315101624),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431607246399),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7664013\n",
      "0.32413524\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('woman', 'man'))\n",
    "print(wv.similarity('woman', 'cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, super. Mam świetny model, mogę nim podpowiadać słowa, wynajdować niepasujące elementy, uzupełniać zdania, znajdować synonimy, itd. Ale czy da się to jakoś wykorzystać do klasyfikacji? word2vec ma wektor na każde słowo - jak z tego zrobić wektor na ciąg słów?\n",
    "\n",
    "**Odpowiedź: można uśrednić znaczenie słów w dokuemncie poprzez zsumowanie wektorów wszystkich słów.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.key_to_index:\n",
    "            vectors = wv.get_normed_vectors()\n",
    "            index = wv.key_to_index[word]\n",
    "            mean.append(vectors[index])\n",
    "            all_words.add(index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.layer_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bardzo szybko spróbujemy zastosować to podejście do predykcji gatunku filmu na podstawie jego opisu. Poniżej kod wczytujący ciekawy zbiór danych oraz pokazujący jakie ma klasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  movieId                                               plot  \\\n",
      "0           0        1  A little boy named Andy loves to be in his roo...   \n",
      "1           1        2  When two kids find and play a magical board ga...   \n",
      "2           2        3  Things don't seem to change much in Wabasha Co...   \n",
      "3           3        6  Hunters and their prey--Neil and his professio...   \n",
      "4           4        7  An ugly duckling having undergone a remarkable...   \n",
      "\n",
      "         tag  \n",
      "0  animation  \n",
      "1    fantasy  \n",
      "2     comedy  \n",
      "3     action  \n",
      "4    romance  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='tag'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('data/tagged_plots_movielens.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "print(df.head())\n",
    "df.tag.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Szybko dzielimy dane na zbiór uczący i testowy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizujemy dane i wyliczamy reprezentację wektorową (za pomocą zsumowanych wektorów słów wrod2vec):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m test_tokenized \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m r: w2v_tokenize_text(r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      2\u001b[0m train_tokenized \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m r: w2v_tokenize_text(r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m----> 3\u001b[0m X_train_word_average \u001b[38;5;241m=\u001b[39m word_averaging_list(wv,train_tokenized)\n\u001b[0;32m      4\u001b[0m X_test_word_average \u001b[38;5;241m=\u001b[39m word_averaging_list(wv,test_tokenized)\n",
      "Cell \u001b[1;32mIn[75], line 20\u001b[0m, in \u001b[0;36mword_averaging_list\u001b[1;34m(wv, text_list)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m  \u001b[38;5;21mword_averaging_list\u001b[39m(wv, text_list):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mvstack([word_averaging(wv, review) \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m text_list ])\n",
      "Cell \u001b[1;32mIn[75], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m  \u001b[38;5;21mword_averaging_list\u001b[39m(wv, text_list):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mvstack([word_averaging(wv, review) \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m text_list ])\n",
      "Cell \u001b[1;32mIn[75], line 9\u001b[0m, in \u001b[0;36mword_averaging\u001b[1;34m(wv, words)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m wv\u001b[38;5;241m.\u001b[39mkey_to_index:\n\u001b[0;32m      8\u001b[0m         vectors \u001b[38;5;241m=\u001b[39m wv\u001b[38;5;241m.\u001b[39mget_normed_vectors()\n\u001b[1;32m----> 9\u001b[0m         mean\u001b[38;5;241m.\u001b[39mappend(vectors[wv\u001b[38;5;241m.\u001b[39mkey_to_index[word]\u001b[38;5;241m.\u001b[39mindex])\n\u001b[0;32m     10\u001b[0m         all_words\u001b[38;5;241m.\u001b[39madd(wv\u001b[38;5;241m.\u001b[39mkey_to_index[word]\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mean:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
    "train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uczymy i testujemy klasyfikator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "\n",
    "# logreg = logreg.fit(X_train_word_average, train_data['tag'])\n",
    "# predicted = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patrzymy jak nam poszło:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Trafność klasyfikacji %s' % accuracy_score(test_data.tag, predicted))\n",
    "# cm = confusion_matrix(test_data.tag, predicted)\n",
    "# print('Macierz pomyłek\\n %s' % cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak na brak porządnego przetwarzania wstępnego, nie jest to zły wynik. Mam nadzieję, że ten przykład pokazał jak można wykorzystać word2vec do tworzenia atrybutów dla problemów klasyfikacyjnych."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
