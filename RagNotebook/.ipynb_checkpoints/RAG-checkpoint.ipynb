{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0811dab-f855-475a-a7ab-2dfce0f5a2be",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Ten notatnik pomoże Ci zapoznać się z podstawowym podejściem do Retrieval Augmented Generation (RAG). W trakcie ćwiczenia będziemy korzystać głównie z bibliotek [openai](https://github.com/openai/openai-python), [langchain](https://python.langchain.com/) i [trulens](https://www.trulens.org/). Po uzupełnieniu tego notatnika powinieneś wiedzieć:\n",
    "- czym jest RAG,\n",
    "- jak z poziomu kodu komunikować się z LLMem,\n",
    "- jak pobierać i dzielić dokumenty tekstowe na potrzeby RAG,\n",
    "- jak policzyć zanurzenia i przechowywać zanurzenia dla swoich dokumentów,\n",
    "- jak wykonać sematyczne wyszukiwanie wśród dokumentów,\n",
    "- jak wstrzykiwać kontekst do zapytań do LLMów,\n",
    "- jak prowadzić dłuższe konwersacje z LLMem,\n",
    "- jak mierzyć i monitorować jakość odpowiedzi LLMów.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6735ec64-5f8e-42de-903c-2584069a847c",
   "metadata": {},
   "source": [
    "## Przygotowanie\n",
    "\n",
    "Do pracy z LLMami potrzebny będzie klucz API do dostawcy takowych. Na tych zajęciach będziemy łączyć się z modelami firmy OpenAI. Jeśli jeszcze nie masz klucza API, skorzystaj z instrukcji na stronie: https://platform.openai.com/docs/quickstart?context=python.\n",
    "\n",
    "Oprócz tego będziemy potrzebować kilku bibliotek:\n",
    "\n",
    "```{command}\n",
    "pip install openai langchain langchain_openai pypdf youtube-transcript-api chromadb trulens_eval\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da20b87-4213-4184-9ed8-a672b5559454",
   "metadata": {},
   "source": [
    "Jeśli masz potrzebne biblioteki, możesz sprawdzić czy jesteś w stanie komunikować się z LLMem. Wykonaj poniższy kod, żeby sprawdzić czy wszystko działa. Jeśli nie masz klucza API w zmiennej systemowej, po prostu wklej go do kodu poniżej zamiast odwołania do zmiennej systemowej. Jeśli chcesz pracować ze zmienną systemową, prawdopodobnie będziesz musiał po dodaniu zmiennej systemowej uruchomić nowy terminal, w którym nowa zmienna będzie widoczna, ponownie odpalić serwer jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33419a0-a224-4a49-b725-fe8c7940f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f30f0-2627-46ea-a00e-b7ecd923cbb1",
   "metadata": {},
   "source": [
    "**Zad. 1: Uruchom poniższy kod. Możesz zmodyfikować opis systemu i zapytanie.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8986db69-97a4-4db4-b1b0-d6ac8dbae779",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI()\n\u001b[0;32m      3\u001b[0m completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      4\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m   messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m   ]\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\zed\\Lib\\site-packages\\openai\\_client.py:92\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m     90\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     )\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in songwriting.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write lyrics for a country song about Polish winters.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e1c1e-1827-4a69-8b28-1f439c01b1d9",
   "metadata": {},
   "source": [
    "_Jeśli masz problemy z uzyskaniem odpowiedzi, sprawdź czy masz źródła na koncie powiązanym z kluczem API: https://platform.openai.com/account/billing/overview. Na tej samej stronie możesz zobaczyć ile zostało Ci funduszy. Podczas tych zajęć nie powinniśmy wydać więcej niż kilkanaście centów._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b30d7-34ba-46de-adca-f2283f69e9cb",
   "metadata": {},
   "source": [
    "Oprócz informacji zwrotnej od LLMa mamy również szereg metadanych, w tym ile tokenów wysłaliśmy a ile odebraliśmy.\n",
    "\n",
    "**Zad. 2: Wiedząc, że dla GPT-3.5-turbo koszt wysyłanych danych to 0.001 USD/1K tokenów, a koszt odpowiedzi to 0.002 USD/1K tokenów, policz ile kosztowało Cię powyższe zapytanie korzystając z pola `completion.usage`. _Jeśli ktoś chce korzystać z GPT-4 Turbo (gpt-4-1106-preview) to koszty wysłanych i odebranych tokenów to kolejno 0.01 i 0.03 USD/1K tokenów._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da42b1-5b43-49dc-b15b-8f3e6aa26f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a6b6273-459d-49aa-9faa-53016a9f3b20",
   "metadata": {},
   "source": [
    "Na koniec zapiszemy sobie treść wyniku, może się jeszcze przyda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9b290a-5cd3-48f7-9479-046bd6cc06ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'completion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m song \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "\u001b[1;31mNameError\u001b[0m: name 'completion' is not defined"
     ]
    }
   ],
   "source": [
    "song = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5fa74-31cb-4552-aa04-94eda6640b30",
   "metadata": {},
   "source": [
    "## Wgrywanie danych\n",
    "\n",
    "Langchain (jak i inne narzędzia, np. LlamaIndex) posiadają szereg funkcji pomocniczych do zdobywania tekstu z filmów, podkastów, baz danych, stron internetowych i innych źródeł. Na potrzeby tych zajęć, jako kontekst wykorzystamy dwie krótkie książki Andrew Ng w formacie PDF i napisy z dwóch filmów na Youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b991b89-b0e5-4afb-8524-4d1537e97b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytujemy książki\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"./books\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f761f8-5f35-4ae5-8f29-a0f911fab497",
   "metadata": {},
   "source": [
    "PDFy są domyślnie dzielone na strony, zatem zmienna `docs` to lista stron. Zobaczmy ile stron mają łącznie obie książki i co jest na stronie 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb08129-fead-4a79-a807-65de986b90f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Łacznie w książka jest 159 stron.\n",
      "\n",
      "Zawartość 23. strony w kolekcji to:\n",
      "PAGE 23Each project is only one step on a longer journey, hopefully one that has a positive impact. In addition:\n",
      "Don’t worry about starting too small. One of my first machine learning research projects involved \n",
      "training a neural network to see how well it could mimic the sin(x) function. It wasn’t very useful, but \n",
      "was a great learning experience that enabled me to move on to bigger projects.\n",
      "Building a portfolio of projects, especially one \n",
      "that shows progress over time from simple to \n",
      "complex undertakings, will be a big help when \n",
      "it comes to looking for a job.Communication is key.  You need to be able to explain your thinking if you want others to see \n",
      "the value in your work and trust you with resources that you can invest in larger projects. To get \n",
      "a project started, communicating the value of what you hope to build will help bring colleagues, \n",
      "mentors, and managers onboard — and help them point out flaws in your reasoning. After you’ve \n",
      "finished, the ability to explain clearly what you accomplished will help convince others to open the \n",
      "door to larger projects.\n",
      "Leadership isn’t just for managers.  When you reach the point of working on larger AI projects that \n",
      "require teamwork, your ability to lead projects will become more important, whether or not you are \n",
      "in a formal position of leadership. Many of my friends have successfully pursued a technical rather \n",
      "than managerial career, and their ability to help steer a project by applying deep technical insights \n",
      "— for example, when to invest in a new technical architecture or collect more data of a certain type \n",
      "— allowed them to grow as leaders and also helped significantly improve the project.Building a Portfolio of Projects That Shows Skill Progression CHAPTER 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Łacznie w książka jest {len(docs)} stron.\")\n",
    "print()\n",
    "print(f\"Zawartość 23. strony w kolekcji to:\\n{docs[22].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff312339-0bfd-47f7-8b79-f13ce7c32e5e",
   "metadata": {},
   "source": [
    "Teraz pobierzemy napisy z dwóch filmików gdzie przemawia Andrew Ng. Zwróć uwagę na parametr language - pozwala nam on priorytetyzować ręcznie sporządzone napisy dostarczone przez twórcę filmu (automatycznie wygenerowane napisy w przypadku pierwszego filmu nie są idealne...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f520c10-16f0-4a8d-9772-6677713d2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "clips = []\n",
    "\n",
    "for link in [\"https://www.youtube.com/watch?v=5p248yoa3oE\",\n",
    "             \"https://www.youtube.com/watch?v=0jspaMLxBig\"]:\n",
    "    loader = YoutubeLoader.from_youtube_url(link, add_video_info=False, language=[\"en-US\", \"en-GB\", \"en\"])\n",
    "    clips.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e3d47-fc72-4fd7-bcf8-0f25a701a9c0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Zad. 3: Zobacz ile fragmentów (obiektów typu Document) mają łącznie teksty obu filmików i co jest w transkrypcie o indeksie 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b4e826d-68e3-41cf-9f03-ac192f3790e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[MUSIC PLAYING] It is my pleasure to welcome\\nDr. Andrew Ng, tonight. Andrew is the managing\\ngeneral partner of AI Fund, founder of DeepLearning.AI\\nand Landing AI, chairman and\\nco-founder of Coursera, and an adjunct professor\\nof Computer Science, here at Stanford. Previously, he had started\\nand led the Google Brain team, which had helped\\nGoogle adopt modern AI. And he was also director\\nof the Stanford AI lab. About eight million people, 1\\nin 1,000 persons on the planet, have taken an AI class from him. And through, both, his\\neducation and his AI work, he has changed numerous lives. Please welcome Dr. Andrew Ng. [APPLAUSE] Thank you, Lisa. It's good to see everyone. So, what I want to do\\ntoday is chat to you about some opportunities in AI. So I've been saying AI\\nis a new electricity. One of the difficult things\\nto understand about AI is that it is a general\\npurpose technology, meaning that it's not\\nuseful only for one thing but it's useful for lots\\nof different applications, kind of like electricity. If I were to ask you, what\\nis electricity good for? It's not any one thing,\\nit's a lot of things. So what I'd like to do is\\nstart off sharing with you how I view the\\ntechnology landscape, and this will lead into\\nthe set of opportunities. So lot of hype, lot of\\nexcitement about AI. And I think, a good\\nway to think about AI is as a collection of tools. So this includes, a technique\\ncalled supervised learning, which is very good at\\nrecognizing things or labeling things, and generative AI, which\\nis a relatively new, exciting development. If you're familiar with AI, you\\nmay have heard of other tools. But I'm going to talk less\\nabout these additional tools, and I'll focus today on\\nwhat I think are, currently, the two most important tools,\\nwhich are supervised learning and generative AI. So supervised learning is\\nvery good at labeling things or very good at computing\\ninput to outputs or A to B mappings, given an input\\nA, give me an output. For Example, given an\\nemail, we can use supervised learning to label it\\nas spam or not spam. The most lucrative\\napplication of this that I've ever worked on is\\nprobably online advertising, where given an ad, we\\ncan label if a user likely to click on\\nit, and therefore, show more relevant ads. For self-driving cars, given\\nthe sensor readings of a car, we can label it with\\nwhere are the other cars. One project, that my\\nteam, AI Fund, worked on was ship route optimization. Where given a route the ship is\\ntaking or considering taking, we can label that\\nwith how much fuel we think this will consume,\\nand use this to make ships more fuel efficient. Did a lot of work in automated\\nvisual inspection in factories. So you can take a picture\\nof a smartphone, that was just manufactured and\\nlabel, is there a scratch or any other defect in it. Or if you want to build a\\nrestaurant review, reputation monitoring system, you can\\nhave a little piece of software that looks at online\\nrestaurant reviews, and labels that as positive\\nor negative sentiment. So one nice thing, one cool\\nthing about supervised learning is that it's not useful for\\none thing, it's useful for all of these different applications,\\nand many more, besides. Let me just walk\\nthrough, concretely, the workflow one example\\nof a supervised learning, labeling things kind of project. If you want to build a system\\nto label restaurant reviews, you then collect a few data\\npoints or collect a data set. Where it say, the\\npastrami sandwich great, say that is positive. Servers are slow,\\nthat's negative. My favorite chicken\\ncurry, that's positive. And here, I've shown\\nthree data points, but you are building\\nthis, you may get thousands of\\ndata points like this or thousands of training\\nexamples, we call it. And the workflow of a machine\\nlearning project, of an AI project is, you get\\nlabeled data, maybe thousands of data points. Then you have an AI\\nentry team train an AI model to learn from this data. And then finally,\\nyou would find, maybe a cloud service to\\nrun the trained AI model. And then you can feed it,\\nbest bubble tea I've ever had, and that's positive sentiment. And so, I think the\\nlast decade was maybe the decade of large scale\\nsupervised learning. What we found, starting\\nabout 10, 15 years ago was if you were to\\ntrain a small AI model, so train a small neural\\nnetwork or small deep learning algorithm, basically,\\na small AI model, maybe not on a very\\npowerful computer, then as you fed it more\\ndata, its performance would get better\\nfor a little bit but then it would flatten out. It would plateau,\\nand it would stop being able to use the data\\nto get better and better. But if you were to train a very\\nlarge AI model, lots of compute on maybe powerful GPUs, then as\\nwe scaled up the amount of data we gave the machine\\nlearning model, its performance\\nwould kind of keep on getting better and better. So this is why when I started\\nand led the Google Brain team, the primary mission that\\nI directed the team to solve, at the time, was let's\\njust build really, really large neural networks,\\nthat we then fed a lot of data to. And that recipe,\\nfortunately, worked. And I think the idea of\\ndriving large compute and large scale of data, that\\nrecipe's really helped us, driven a lot of AI progress\\nover the last decade. So if that was the\\nlast decade of AI, I think this decade\\nis turning out to be also doing everything\\nwe had in supervised learning but adding to it the\\nexciting tool of generative AI. So many of you,\\nmaybe all of you, have played with ChatGPT\\nand Bard, and so on. But just given a piece of\\ntext, which you call a prompt, like I love eating, if you\\nrun this multiple times, maybe you get bagels cream\\ncheese or my mother's meatloaf or out with friends,\\nand the AI system can generate output like that. Given the amounts of buzz and\\nexcitement about generative AI, I thought I'd take just half\\na slide to say a little bit about how this works. So it turns out that generative\\nAI, at least this type of text generation, the core of\\nit is using supervised learning that inputs output\\nmappings to repeatedly predict the next word. And so, if your system\\nreads, on the internet, a sentence like, my favorite\\nfood is a bagel with cream cheese and lox, then this is\\ntranslated into a few data points, where if it sees,\\nmy favorite food is A, in this case, try to guess that\\nthe right next word was bagel or my favorite food\\nis a bagel, try to guess the next word\\nis with, and similarly, if it sees that, in this\\ncase, the right guess for the next word\\nwould have been cream. So by taking texts that\\nyou find on the internet or other sources, and by using\\nthis input, output, supervised learning to try to repeatedly\\npredict the next word, if you train a very large AI\\nsystem on hundreds of billions of words, or in the case\\nof the largest models, now more than a trillion words, then\\nyou get a large language model like ChatGPT. And there are additional, other\\nimportant technical details. I talked about\\npredicting the next word. Technically, these\\nsystems predict the next subword or part\\nof a word called a token, and then there are other\\ntechniques like RLHF for further tuning the AI output\\nto be more helpful, honest, and harmless. But at the heart of it\\nis this using supervised learning to repeatedly\\npredict the next word. That's really what's\\nenabling the exciting, really fantastic progress on\\nlarge language models. So while many people have\\nseen large language models as a fantastic consumer tool. You can go to a website\\nlike ChatGPT's website or Bard's or other\\nlarge language models and use it as a fantastic tool. There's one other trend, I\\nthink is still underappreciated, which is the power of\\nlarge language models, not just as a consumer tool\\nbut as a developer tool. So it turns out that\\nthere are applications that used to take me months\\nto build, that a lot of people can now build much faster by\\nusing a large language model. So specifically, the workflow\\nfor supervised learning, building the restaurant\\nreview system, say, would be that you need to\\nget a bunch of labeled data, and maybe that takes a month, we\\nget a few thousand data points. And then have an AI\\nteam train, and tune, and really get optimized\\nperformance on your AI model. Maybe that'll take three months. Then find a cloud\\nservice to run it. Make sure it's running robustly. Make sure it's\\nrecognized, maybe that'll take another three months. So pretty realistic timeline\\nfor building a commercial grade machine learning system\\nis like 6 to 12 months. And so teams I've led, we often\\ntook roughly 6 to 12 months to build and deploy\\nthese systems. And some of them turned\\nout to be really valuable. But this is a realistic timeline\\nfor building and deploying a commercial grade AI system. In contrast, with prompt-based\\nAI, where you write a prompt. This is what the\\nworkflow looks like. You can specify a prompt, that\\ntakes maybe minutes or hours. And then, you can\\ndeploy it to the cloud, and that takes\\nmaybe hours or days. So there are now\\ncertain AI applications that used to take me,\\nliterally, six months, maybe a year to build, that\\nmany teams around the world can now build in maybe a week. And I think this is\\nalready starting, but the best is\\nstill yet to come. This is starting to open\\nup a flood of a lot more AI applications that can be\\nbuilt by a lot of people. So I think many people\\nstill underestimate the magnitude of the flood\\nof custom AI applications that I think is going\\nto come down the pipe. Now, I know you\\nprobably were not expecting me to write\\ncode in this presentation, but that's what I'm going to do. So it turns out,\\nthis is all the code that I need in order to\\nwrite a sentiment classifier. So I'm going to-- some of you will\\nknow Python, I guess. Import some tools\\nfrom OpenAI, and then add this prompt, that says,\\nclassify the text below delimited by three\\ndashes as having either a positive or\\nnegative sentiment. [INAUDIBLE],, I had a fantastic\\ntime at Stanford GSB. Learnt a lot, and also\\nmade great new friends. All right. So that's my prompt. And then I'm just\\ngoing to run it. And I've never run it before. So I really hope-- thank goodness, it\\ngot the right answer. [APPLAUSE] And this is literally\\nall the code it takes to build a\\nsentiment classifier. And so, today, developers\\naround the world can take, literally,\\nmaybe 10 minutes to build a system like this. And that's a very\\nexciting development. So one of the things\\nI've been working on was trying to teach\\nonline classes about how to use prompting, not\\njust as a consumer tool but as a developer too. So just talking about\\nthe technology landscape, let me now share my\\nthoughts on what are some of the AI opportunities I see. This shows what I think is\\nthe value of different AI technologies today, and\\nI'll talk about three years from now. But the vast majority of\\nfinancial value from AI today is, I think,\\nsupervised learning, where for a single\\ncompany like Google can be worth more than\\n$100 billion US a year. And also, there are\\nmillions of developers building supervised\\nlearning applications. So it's already massively\\nvaluable, and also with tremendous momentum\\nbehind it just because of the sheer effort in\\nfinding applications and building applications. And then, generative AI is the\\nreally exciting new entrant, which is much smaller right now. And then, there\\nare the other tools that I'm including\\nfor completeness. If the size of these circles\\nrepresent the value today, this is what I think it\\nmight grow to in three years. So supervised learning,\\nalready really massive, may double, say,\\nin the next three years, from truly massive\\nto even more massive. And generative AI, which is\\nmuch smaller today, I think, will much more than double in\\nthe next three years because of the number-- the amount\\nof developer interest, the amount of venture\\ncapital investments, the number of large corporates\\nexploring applications. And I also just\\nwant to point out, three years is a very\\nshort time horizon. If it continues to compound\\nin anything near this rate, then in six years, it will\\nbe even vastly larger. But this light shaded\\nregion in green or orange, that light shaded region\\nis where the opportunity is for either new startups or for\\nlarge companies, incumbents, to create and to\\nenjoy value capture. But one thing I hope you\\ntake away from this slide is that all of\\nthese technologies are general purpose\\ntechnologies. So in the case of\\nsupervised learning, a lot of the work that had to\\nbe done over the last decade, but is continuing\\nfor the next decade, is to identify and to execute\\non the concrete use cases. And that process is also\\nkicking off for generative AI. So for this part of\\nthe presentation, I hope you take away from it\\nthat general purpose technology is a useful for many\\ndifferent tasks, lot of value remains to be created\\nusing supervised learning. And even though, we're nowhere\\nnear finishing figuring out the exciting use cases\\nof supervised learning, we have this other fantastic\\ntool of generative AI, which further expands the set of\\nthings we can now do using AI. But one caveat, which\\nis that there will be short term fads along the way. So I don't know if\\nsome of you might remember the app called Lensa. This is the app\\nthat will let you upload pictures of\\nyourself, and then will render a cool\\npicture of you as an astronaut or a\\nscientist or something. And it was a good idea\\nand people liked it. And its revenue just took\\noff like crazy like that, through last December. And then it did that. And that's because Lensa\\nwas-- it was a good idea. People liked it. But it was a relatively\\nthin software layer on top of someone else's\\nreally powerful APIs. And so even though it\\nwas a useful product, it was in a defensible business. And when I think\\nabout apps like Lensa, I'm actually reminded of when\\nSteve Jobs gave us the iPhone. Shortly after,\\nsomeone wrote an app that I paid $1.99 for, to\\ndo this, to turn on the LED, to turn the phone\\ninto a flashlight. And that was also a good\\nidea to write an app to turn on the LED\\nlight, but it also wasn't a defensible long term-- also didn't create\\nvery long term value because it was easily\\nreplicated, and underpriced, and eventually\\nincorporated into iOS. But with the rise of iOS,\\nwith the rise of iPhone, someone also figured out how\\nto build things like Uber, and Airbnb, and Tinder. The very long term, very\\ndefensible businesses that created sustaining value. And I think, with the\\nrise of generative AI or the rise of new AI\\ntools, I think, really, what excites me\\nis the opportunity to create those really deep,\\nreally hard applications that hopefully can create\\nvery long term value. So the first trend\\nI want to share is AI is a general\\npurpose technology. And a lot of the work\\nthat lies ahead of us, is to find the very diverse\\nuse cases and to build them. There's a second\\ntrend I want to share with you, which relates to why\\nAI isn't more widely adopted yet. It feels like a bunch of us\\nhave been talking about AI for 15 years or something. But if you look at where\\nthe value of AI is today, a lot of it is still very\\nconcentrated in consumer software internet. Once you got outside tech or\\nconsumer software internet, there's some AI adoption\\nbut it all feels very early. So why is that? It turns out, if\\nyou were to take all current and\\npotential AI projects, and sort them in\\ndecreasing order of value, then to the left of this curve,\\nof the head of this curve, are the multi-billion dollar\\nprojects like advertising or web search or for e-commerce\\nproduct recommendations or company like Amazon. And it turns out that\\nabout 10, 15 years ago, [? there's ?] my\\nfriends and I, we figured out a recipe\\nfor how to hire, say, 100 engineers to write\\none piece of software to serve more relevant\\nads, and apply that one piece of software\\nto a billion users, and generate massive\\nfinancial value. So that works. But once you go outside\\nconsumer software internet, hardly anyone has 100\\nmillion or a billion users that you can write and apply\\none piece of software to. So once you go to\\nother industries, as we go from the head\\nof this curve on the left over to the long tail, these\\nare some of the projects I see, and I'm excited about. I was working with\\na pizza maker that was taking pictures of the pizza\\nthey were making because they needed to do things like make\\nsure that the cheese is spread evenly. So this is about a\\n$5 million project. But that recipe of hiring a\\nhundred engineers or dozens of engineers to\\nwork on a $5 million project, that\\ndoesn't make sense. Or there's another\\ngreat example. Working with a\\nagriculture company that with them, we figured\\nout that if we use cameras to find out how\\ntall is the wheat, and wheat is often\\nbent over because of wind or rain or\\nsomething, and we can chop off the wheat\\nat the right height, then that results in more\\nfood for the farmer to sell, and is also better\\nfor the environment. But this is another\\n$5 million project, that that old recipe of\\nhiring a large group of highly skilled engineers to work\\non this one project, that doesn't make sense. And similarly materials\\ngrading, cloth grading, sheet metal grading,\\nmany projects like this. So whereas to the left,\\nin the head of this curve, there's a small\\nnumber of, let's say, multi-billion dollar\\nprojects, and we know how to execute\\nthose delivering value. In other industries, I'm\\nseeing a very long tail of tens of thousands,\\nof let's call them, $5 million projects,\\nthat until now, have been very\\ndifficult to execute on because of the high\\ncost of customization. The trend that I\\nthink is exciting is that the AI community has\\nbeen building better tools that lets us aggregate\\nthese use cases, and make it easy for the end\\nuser to do the customization. So specifically,\\nI'm seeing a lot of exciting low code\\nand no code tools, that enable the user to\\ncustomize the AI system. What this means\\nis instead of me, needing to worry that much\\nabout pictures of pizza, we have tools-- we're starting to see\\ntools that can enable the IT department of\\nthe pizza making factory to train AI system on\\ntheir own pictures of pizza to realize this $5\\nmillion worth of value. And by the way, the\\npictures of pizza, they don't exist\\non the internet. So Google and Bing don't have\\naccess to these pictures, we need tools that can\\nbe used by, really, the pizza factory themselves,\\nto build, and deploy, and maintain their own\\ncustom AI system that works on their own pictures of pizza. And broadly, the technology\\nfor enabling this, some of it is prompting, text\\nprompting, visual prompting, but really, large language\\nmodels and similar tools like that or a technology\\ncalled data-centric AI, whereby, instead of asking the pizza\\nfactory to write a lot of code, which is challenging, we can\\nask them to provide data which turns out to be more feasible. And I think the second\\ntrend is important, because I think this is a key\\npart of the recipe for taking the value of AI, which\\nso far still feels very concentrated in the tech\\nworld and the consumer software internet world, and pushing this\\nout to all industries, really to the rest of the\\neconomy, which-- sometimes it's easy to forget,\\nthe rest of the economy is much bigger than\\nthe tech world. So the two trends I shared,\\nAI as a general purpose technology, lots of\\nconcrete use cases to be realized as well as low\\ncode, no code, easy to use tools, enabling AI to be\\ndeployed in more industries. How do we go after\\nthese opportunities? So about five years ago, there\\nwas a puzzle I wanted to solve, which is-- I felt that many valuable AI\\nprojects are now possible. And I was thinking, how\\ndo we get them done? And having led teams in\\nGoogle, and Baidu, in big tech companies, I had a\\nhard time figuring out how I could operate a\\nteam in a big tech company to go after a very diverse set\\nof opportunities in everything from maritime shipping to\\neducation to financial services to healthcare, and on and on. It's just very diverse\\nuse cases, very diverse go to markets, and very\\ndiverse customer bases and applications. And I felt that the\\nmost efficient way to do this would\\nbe if we can start a lot of different companies\\nto pursue these very diverse opportunities. So that's why I ended up\\nstarting AI Fund, which is a venture studio\\nthat builds startups to pursue a diverse set\\nof AI opportunities. And, of course, in addition\\nto lots of startups, incumbent companies\\nalso have a lot of opportunities to integrate\\nAI into existing businesses. In fact, one pattern I'm seeing\\nfor incumbent businesses is distribution is often one of\\nthe significant advantages of incumbent companies, if\\nthey play their cards right, can allow them to integrate\\nAI into their products, quite efficiently. But just to be concrete,\\nwhere are the opportunities? So I think of this as-- this is\\nwhat I think of as an AI stack. At the bottom level is the\\nhardware, semiconductor layer. Fantastic opportunities there,\\nbut very capital intensive, very concentrated. So needs a lot of resources,\\nrelatively few winners. So some people can\\nand should play there. I personally don't like\\nto play there myself. There's also the\\ninfrastructure layer. Also fantastic opportunities,\\nbut very capital intensive, very concentrated. So I tend not to play\\nthere myself, either. And then there's the\\ndeveloper tool layer. What I showed you just now was-- I was actually using OpenAI's\\nAPI as a developer tool. And then, I think\\nthe developer tool sector is a hypercompetitive. Look at all the startups\\nchasing OpenAI right now. But there will be\\nsome mega winners. And so I sometimes play\\nhere, but primarily, when I think of a meaningful\\ntechnology advantage, because I think that\\nearns you the right or earns you a better shot at\\nbeing one of the mega winners. And then lastly, even though\\na lot of the media attention and the buzz is in the\\ninfrastructure and developer tooling layer, it turns out\\nthat layer can be successful only if the application layer\\nis even more successful. And we saw this with the\\nrise of SaaS as well. Lot of the buzz and excitement\\nis on the technology, the tooling layer. Which is fine. Nothing wrong with that. But the only way for\\nthat to be successful is if the application layer\\nis even more successful, so that, frankly,\\nthey can generate enough revenue to pay the\\ninfrastructure, and the tooling layer. So, actually, let me\\nmention one example. Amorai-- I was actually just\\ntexting the CEO yesterday. But Amorai is a\\ncompany that we built that uses AI for romantic\\nrelationship coaching. And just to point\\nout, I'm an AI guy. And I feel like I know\\nnothing really about romance. And if you don't believe\\nme, you can ask my wife, she will confirm that I\\nknow nothing about romance. But when we went\\nto build this, we wanted to get together with the\\nformer CEO of Tinder, Renate Nyborg. And with my team's\\nexpertise in AI, and her expertise\\nin relationships because she ran Tinder, she\\nknows more about relationships than I think anyone\\nI know, we're able to build something\\npretty unique using AI for kind of romantic\\nrelationship mentoring. And the interesting thing\\nabout applications like these is when we look around,\\nhow many teams in the world are simultaneously expert\\nin AI and in relationships? And so at the\\napplication layer, I'm seeing a lot of\\nexciting opportunities that seem to have a\\nvery large market, but where the\\ncompetition sets is very light, relative to the\\nmagnitude of the opportunity. It's not that there\\nare no competitors, but it's just much less intense\\ncompared to the developer tool or the infrastructure layers. And so, because I've spent\\na lot of time iterating on a process of building\\nstartups, what I'm going to do is just, very\\ntransparently, tell you the recipe we've developed\\nfor building startups. And so after many years of\\niteration and improvement, this is how we now\\nbuild startups. My team's always had access\\nto a lot of different ideas, internally generated,\\nideas from partners. And I want to walk through this\\nwith one example of something we did, which is a\\ncompany Bearing AI, which uses AI to make\\nships more fuel efficient. So this idea came to me\\nwhen, a few years ago, a large Japanese\\nconglomerate called Mitsui, that is a major shareholder and\\noperates major shipping lines, they came to me and they\\nsaid, hey, Andrew, you should build a business to\\nuse AI to make ships more fuel efficient. And the specific\\nidea was, think of it as a Google Maps for ships. We can suggest a ship or\\ntell a ship how to steer, so that you still get to\\nyour destination on time, but using, it turns out,\\nabout 10% less fuel. And so what we now do is\\nwe spend about a month, validating the idea. So double check, is this idea\\neven technically feasible, and then talk to\\nprospective customers to make sure there\\nis a market need. So we spent up to about\\na month doing that. And if it passes\\nthis stage, then we will go and recruit a CEO to\\nwork with us on the project. When I was starting,\\nout I used to spend a long time working\\non a project myself, before bringing on a CEO. But after iterating, we\\nrealized that bringing on a leader at\\nthe very beginning to work with us, it reduces\\na lot of the burden of having to transfer knowledge\\nor having a CEO come in and having to revalidate\\nwhat [? we ?] discovered. So the process is, we've,\\nlearned much more efficient, we just bring the leader\\nat the very start. And so in the case\\nof Bearing AI, we found a fantastic\\nCEO, Dylan Keil, who is a reputed entrepreneur,\\none successful exit before. And then we spent three\\nmonths, six, two week sprints, to work with them\\nto build a prototype as well as do deep\\ncustomer validation. If it survives\\nthis stage, and we have about a two thirds,\\n66% survival rate, we then write the\\nfirst check in, which then gives the\\ncompany resources to hire an executive\\nteam, build the key team, get an MVP working, minimum\\nviable product working, and get some real customers. And then after that,\\nhopefully, then successfully raises additional external\\nrounds of funding, and can keep on\\ngrowing and scaling. So I'm really proud of\\nthe work that my team was able to do to support\\nMitsui's idea, and Dylan Keil, as CEO. And today, there are hundreds\\nof ships, on the high seas right now, that are steering\\nthemselves differently because of Bearing AI. And 10% fuel savings\\ntranslates to around to maybe $450,000 in savings in\\nfuel, per, ship per year. And, of course, it's also,\\nfrankly, quite a bit better for the environment. And I think this\\nstartup, I think, would not have existed if not\\nfor Dylan's fantastic work, and then also, Mitsui\\nbringing this idea to me. And I like this example because\\nthis is another one is like-- this is a startup idea\\nthat, just to point out, I would never have\\ncome up with myself. Because I've been\\non a boat but what do I know about\\nmaritime shipping. But is the deep subject\\nmatter expertise of Mitsui, that had this insight,\\ntogether with Dylan, and then my team's expertise\\nin AI, that made this possible. And so as I operate in\\nAI, one thing I've learned is my swim lane is\\nAI, and that's it. Because I don't have time or\\nit's very difficult for me to be expert in\\nmaritime shipping, and romantic relationships,\\nand health care, and financial services,\\nand on, and on, and on. And so I've learned\\nthat if I can just help get a accurate\\ntechnical validation, and then use AI\\nresources to make sure the AI tech is built\\nquickly and well, and I think, we've\\nalways managed to help the companies build\\na strong technical team, then partnering with subject\\nmatter experts often results in exciting new opportunities. And I want to share with you\\none other weird aspect of-- one other weird\\nthing I've learned about building\\nstartups, which is I like to engage only when\\nthere's a concrete idea. And this runs counter to\\na lot of the advice you hear from the design thinking\\nmethodology, which often says, don't rush to solutioning. Explore a lot of alternatives\\nbefore you do a solution. Honestly, we tried\\nthat, it was very slow. But what we've learned\\nis that at the ideation stage, if someone comes to\\nme and says, hey, Andrew, you should apply AI\\nto financial services. Because I'm not a subject matter\\nexpert in financial services, it's very slow for\\nme to go and learn enough about financial services,\\nto figure out what to do. I mean, eventually, you\\ncould get to a good outcome, but it's a very labor\\nintensive, very slow, very expensive process, for\\nme, to try to learn industry after industry. In contrast, one of my\\npartners wrote this idea as a tongue in cheek,\\nnot really seriously. But, let's say,\\n[INAUDIBLE] by GPT, let's eliminate commercials\\nby automatically buying every product advertised\\nin exchange for not having to see any ads, it's\\nnot a good idea, but it is a concrete idea. And it turns out, concrete ideas\\ncan be validated or falsified, efficiently. They also give a team a\\nclear direction to execute. And I've learned that\\nin today's world, especially, with the excitement,\\nthe buzz, the exposure to AI of a lot of people, it\\nturns out that there are a lot of subject matter\\nexperts in today's world, that have deeply thought about\\na problem for months, sometimes even one or two years. But they've not yet\\nhad a build partner. And when we get together\\nwith them, and hear, and they share\\nthe idea of us, it allows us to work with\\nthem to very quickly go into validation and building. And I find that this\\nworks because there are a lot of people that\\nhave already done the design thinking thing of exploring\\na lot of ideas and winnowing down to really good ideas. And there are-- I\\nfind that there are so many good ideas\\nsitting out there, that no one is working on. That finding those good ideas\\nthat someone has already had, and wants to share with us,\\nand wants to build partner for, that turns out to be a\\nmuch more efficient engine. So before I wrap up, we'll go\\nto the question in a second, just a few slides to talk\\nabout risk and social impact. So AI is very\\npowerful technology. To say something you'd\\nprobably guess, my teams and I, we only work on projects\\nthat move humanity forward. And we have multiple\\ntimes killed projects that we assess to be\\nfinancially sound, based on ethical grounds. It turns out, I've\\nbeen surprised and sometimes dismayed at\\nthe creativity of people to come up with good ideas. So to come up with\\nreally bad ideas that seem profitable but really\\nshould not be built. We've killed a few\\nprojects on those grounds. And then, I think, has to be\\nacknowledged that AI today does have problems with\\nbias, fairness, and accuracy. But also the technology\\nis improving quickly. So I see that AI\\nsystems today are less biased than six\\nmonths ago, and more fair than six months\\nago, which is not to dismiss the importance\\nof these problems. They are problems and we should\\ncontinue to work on them. But I'm also gratified\\nat the number of teams working\\nhard on these issues to make them much better. When I think of the\\nbiggest risks of AI. I think that the biggest risks-- one of the biggest risks\\nis the disruption to jobs. This is a diagram from a paper\\nby our friend at the University of Pennsylvania, and\\nsome folks at OpenAI, analyzing the exposure\\nof different jobs to AI automation. And it turns out that, whereas,\\nthe previous wave of automation mainly-- the most exposed jobs were\\noften the lower wage jobs, such as when we put\\nrobots into factories. With this current\\nwave of automation, is actually the higher\\nwage jobs, further, to the right of this\\naxis, that seems to have more of their tasks\\nexposed to automation. So even as we create\\ntremendous value using AI, I feel like, as citizens,\\nand our corporations, and our governments,\\nand, really, our society, I feel\\na strong obligation to make sure that people,\\nespecially people whose livelihoods are disrupted,\\nare still well taken care of, are still treated well. And then lastly,\\nthere's also been-- it feels like every time there's\\na big wave of progress in AI, there's a big wave of hype about\\nartificial general intelligence as well. When DeepLearning started\\nwork really well 10 years ago, there was a lot\\nof hype about AGI. And now, the generative\\nAI is working really well, there's another wave\\nof hype about AGI. But I think that artificial\\ngeneral intelligence, AI that can do\\nanything a human can do is still decades away, maybe 30\\nto 50 years, maybe even longer. I hope we'll see it\\nin our lifetimes. But I don't think\\nthere's any time soon. One of the challenges is\\nthat the biological path to intelligence, like\\nhumans and the digital path to intelligence, AI, they've\\ntaken very different paths. And the funny thing about\\nthe definition of AGI is you're benchmarking this\\nvery different digital path to intelligence with\\nreally the biological path to intelligence. So I think, large language\\nmodels are smarter than any of us in\\ncertain key dimensions, but much dumber than any\\nof us in other dimensions. And so forcing it to do\\neverything a human can do is like a funny comparison. But I hope we'll get there. Hopefully, within our lifetimes. And then there's also\\na lot of, I think, overblown hype about AI creating\\nextinction risks for humanity. Candidly, I don't see it. I just don't see how AI creates\\nany meaningful extinction risk for humanity. I think that people worry\\nwe can't control AI. But we have lots of, AI will be\\nmore powerful than any person. But with lots of experience,\\nsteering, very powerful entities, such as corporations\\nor nation states that are far more powerful\\nthan any single person, and making sure they, for the\\nmost part, benefit humanity. And also technology\\ndevelops gradually. The so-called hot\\ntake off scenario, where it's not\\nreally working today, and then suddenly,\\none day, overnight, it works brilliantly, and we\\nachieve super intelligence, takes over the world. That's just not realistic. And I think the AI technology\\nwill develop slowly, like all the-- and then it gives\\nus plenty of time to make sure that we\\nprovide oversight and can manage it to be safe. And lastly, if you look at\\nthe real extinction risk to humanity, such\\nas, fingers crossed, the next pandemic\\nor climate change, leading to a massive\\nde-population of some parts of the planet, or much lower\\nodds, but maybe someday, an asteroid doing to us what\\nit had done to the dinosaurs. I think if we look at the\\nactual real extinction risk to humanity, AI\\nhaving more intelligence, even artificial\\nintelligence in the world, would be a key part\\nof the solution. So I feel like if you want\\nhumanity to survive and thrive for the next 1,000 years,\\nrather than slowing AI down, which some people\\npropose, I would rather make AI go as fast as possible. So with that, just to summarize,\\nthis is my last slide. I think that AI, as a\\ngeneral purpose technology creates a lot of new\\nopportunities for everyone. And a lot of the exciting\\nand important work that lies ahead of us all is to go\\nand build those concrete use cases, and hopefully,\\nin the future, hopefully, I'll have\\nopportunities to maybe engage with more of you on\\nthose opportunities as well. So with that, let me just\\nsay, thank you all very much. [APPLAUSE]\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(clips))\n",
    "clips[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb806455-7fc5-46bc-8e50-e1b3b4f31f82",
   "metadata": {},
   "source": [
    "Na koniec rozszerzemy tekst z książek o transkrypty z filmów, żeby mieć jedną wspólną listę kontekstów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf20590d-c312-4276-b0f7-13c823c62140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    }
   ],
   "source": [
    "docs.extend(clips)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6594a-f19e-4150-97a3-3ed383c07787",
   "metadata": {},
   "source": [
    "## Dzielenie tekstu na części\n",
    "\n",
    "Na temat dzielenia tekstu na mniejsze fragmenty można by przygotować osobny tutorial. Można dzielić na podstawie znaków, tokenów, parsować zdania za pomocą nltk, wykrywać akapity i rozdziały, tworzyć hierarchie fragmentów. Przykłady bardziej zaawansowanych technik z wykorzystaniem LlamaIndex można znaleźć na [blogach](https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b) i [darmowych kursach](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/).\n",
    "\n",
    "W ramach tych zajęć skorzystamy tylko z jednego prostego podejścia do dzielenia tekstu opartego na wybranych znakach. `RecursiveCharacterTextSplitter`. bo tak nazywa się klasa z której skorzystamy, stara się dzielić tekst na mniejsze części o zadanej długości. W tym celu wyszukuje wskazanych znaków i wybiera pierwszy, który pozwoli uzyskać fragment nie dłuższy niż wskazana liczba znaków. Zobacz jak to działa na przykładzie poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d1c4409-fd37-4547-a017-3067188cc376",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'song' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      3\u001b[0m r_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[0;32m      4\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      5\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      6\u001b[0m     separators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m r_splitter\u001b[38;5;241m.\u001b[39msplit_text(song)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'song' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "song = \"\"\"\n",
    "I've had a largemouth bass, bust my line\n",
    "A couple of beautiful girls tell me goodbye\n",
    "Trucks break down, dogs run off\n",
    "Politicians lie, been fired by the boss\n",
    "It takes one hand\n",
    "To count the things I can count on\n",
    "No, there ain't much, man\n",
    "That ain't ever let me down\n",
    "Longneck, ice cold beer never broke my heart\n",
    "Like diamond rings and football teams\n",
    "Have torn this boy apart\n",
    "Like a neon dream it just dawned on me\n",
    "The bars and this guitar\n",
    "And longneck, ice cold beer never broke my heart\n",
    "She was a Carolina blue jean baby\n",
    "Fire in her eyes that drove me crazy\n",
    "It was red tail lights when she left town\n",
    "If I didn't know then, I sure know now\n",
    "Longneck, ice cold beer never broke my heart\n",
    "Like diamond rings and football teams\n",
    "Have torn this boy apart\n",
    "Like a neon dream it just dawned on me\n",
    "The bars and this guitar\n",
    "And longneck, ice cold beer never broke my heart\n",
    "It takes one hand\n",
    "To count the things I can count on\n",
    "But I got one hand\n",
    "It's gripping down on a cold one\n",
    "'Cause longneck, ice cold beer never broke my heart\n",
    "Like diamond rings and football teams\n",
    "Have torn this boy apart\n",
    "Like a neon dream it just dawned on me\n",
    "The bars and this guitar\n",
    "And longneck, ice cold beer never broke my heart, no\n",
    "It never broke my heart\n",
    "\"\"\"\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \", \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b5e5164-61fd-45f5-bb94-aa3b6ae2efbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'song' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m r_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[0;32m      2\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      3\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      4\u001b[0m     separators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 6\u001b[0m r_splitter\u001b[38;5;241m.\u001b[39msplit_text(song)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'song' is not defined"
     ]
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=10,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \", \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b33ec4-23e9-4184-a543-da256b8c5ed1",
   "metadata": {},
   "source": [
    "**Zad. 4: Podziel dokumenty w zmiennej `docs` na części o długości 750 z zakładką o rozmiarze 150. Możesz do tego użyć fukcji `split_documents` zamiast `split_text`. Wynik przypisz do zmiennej `splits`. Sprawdź ile fragmentów zawiera `splits` i porównaj to z liczbą elementów w `docs`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e65f4-6fde-40df-bc28-2cda3b8e3330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115fea0-88fe-4371-bb7b-b016c53375e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4be0515d-50ba-45b8-b5ae-668f7c17fbc1",
   "metadata": {},
   "source": [
    "## Tworzenie i przechowywanie zanurzeń"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0661c-bc88-4b23-bc44-acb8714ce9b9",
   "metadata": {},
   "source": [
    "Jak wiadomo z wykładów, tekst można zapisywać do baz w różnorakich formatach, jednak dominują formaty wektorowe. Pracując z LLMami będzie nam zależeć na wyszukiwaniu semantycznym, czyli opratnym na znaczeniu tekstu a nie na występowaniu konkretnych słów. Użyjemy zanurzeń od OpenAI, ale można równie dobrze korzystać z zanurzeń udostępnianych na HuggingFace (no. BAAI/bge-small-en-v1.5 albo BAAI/bge-large-en-v1.5) i liczyć je lokalnie na komputerze.\n",
    "\n",
    "Wykonaj poniższy kod, aby policzyć zanurzenia dla przykładów z wykładu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0749277-1645-4f60-b04e-1e75197409fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "sentence1 = \"Ja lubię eksplorację danych.\"\n",
    "sentence2 = \"Ja lubię pływać.\"\n",
    "sentence3 = \"Ja uwielbiam biegać.\"\n",
    "\n",
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631eb324-eb7c-40e3-8444-4b3a1f48db35",
   "metadata": {},
   "source": [
    "**Zad. 5: Podejrzyj jak wygląda takie zanurzenie i jaką ma długość. Następnie policz odległość między każdą parą zanurzeń. Czy zanurzenia 2 i 3 są do siebie bardziej podobne niż pozostałe pary?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102fe40-2e67-4572-beaf-665d73ac8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embedding2))\n",
    "\n",
    "print(np.dot(embedding1, embedding2))\n",
    "print(np.dot(embedding1, embedding3))\n",
    "print(np.dot(embedding2, embedding3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ebb65d-6457-499e-becd-ab3ef2d14b81",
   "metadata": {},
   "source": [
    "Gdy już wiemy jak działa zanurzenia i mamy odpowiedni obiekt do tworzenia takowych w zmiennej `embedding`, czas stworzyć bazę danych dla naszych tekstów. Opcji jest wiele, ale my skorzystamy z bazy Chroma, która potrafi działać w pamięci jak i szybko stworzyć małą bazę sqlite lokalnie na dysku.\n",
    "\n",
    "Uruchom poniższy kod, aby stworzyć bazę zanurzeń, zapisać ją na dysk i zobaczyć ile elementów ma w środku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d411544-863c-4c04-8463-9ddaea4cd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#!rm -rf ./chroma\n",
    "persist_directory = './chroma/'\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b66f9f-a429-4d43-822d-e8d03a3534d7",
   "metadata": {},
   "source": [
    "Po stworzeniu bazy możemy wypróbować wspomniane wyszukiwanie semantyczne. Poniżej napiszemy treść przykładowego zapytania i poprosimy o 3 najbardziej pasujące fragmenty z bazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e99e4-a75d-4bee-81fd-cd8d0d982508",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is an eyeball dataset?\"\n",
    "relevant_splits = vectordb.similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76859aa7-6fc1-4e37-9f97-f0f4a360ebe5",
   "metadata": {},
   "source": [
    "**Zad. 6: Sprawdź ile fragmentów zwróciła baza. Co jest w tych fragmentach? Z których książek lub filmów one pochodzą?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff0dfb-e3c5-4172-94e6-04c0feafa6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32780ee7-7888-471d-aff7-6c5912186e96",
   "metadata": {},
   "source": [
    "Powyżej zapytanie semantyczne zwróciło obiecujące wyniki. Czasami niestety to nie wystarcza. W przypadku duplikatów w bazie, trzeba dbać o różnorodność zwracanych fragmentów. W innych przypadkach trzeba oprócz wyszukiwania semantycznego ograniczyć się do wybranych dokumentów bo są one wprost wskazane w pytaniu. Tymi rzeczami nie mamy czasu się zajmować, ale w zależności od potrzeb można takie problemy rozwiązywać odrobiną technik z tradycyjnych baz danych lub poprosić LLM o pomoc przy zapytaniu do bazy kontekstów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fda04-5cab-4a77-8fd2-261f05466c45",
   "metadata": {},
   "source": [
    "## Odpowiadanie na pytania z wykorzystaniem kontekstu (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f760051-25b2-4db9-9e67-491fca2d4294",
   "metadata": {},
   "source": [
    "Po zebraniu dokumentów, podzieleniu ich na mniejsze fragmenty, policzeniu zanurzeń i zapisaniu ich do bazy, możemy przejść do RAG. To jest moment w, którym błyszczy langchain. Langchain pozwala tworzyć strumienie wywołań różnych narzędzi i przekazywać wyniki jednego narzędzia jako wejście do innego. W tym wypadku przekażemy zapytanie do bazy zanurzeń a następnie zapytanie wraz fragmentami z bazy przekażemy do LLMa. Taki prosty łańcuch wywołań można stworzyć korzystając z kodu poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b490dab-d206-4c2c-8259-74de8b375d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "import warnings \n",
    "warnings.simplefilter(\"ignore\") # API zmienia się bardzo szybko i co rusz coś staje się deprecated. Wyciszymy ostrzeżenia.\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) #Niska temperatura = mało losowości w odpowiedzi LLMa\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad01df6-b177-4ecd-9bdf-4494a8f4e820",
   "metadata": {},
   "source": [
    "Mając taki prosty łąncuch wywołań możemy zadać zapytanie do LLMa licząc, że skorzysta z wyszukanego tekstu podczas udzielania odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90594ab-e46e-4354-9c0d-6a3b84089b03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"What is an eyeball dataset?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da32e88-7dbf-4dfe-b979-49e842f6ed03",
   "metadata": {},
   "source": [
    "_Ciekawscy mogą teraz uruchomić ChatGPT w osobnym oknie i zobaczyć jak LLM odpowiedziałby na to samo pytanie bez znajomości kontekstu._\n",
    "\n",
    "Rzeczy, które mogą się wydarzyć jest znacznie więcej. Jedną z istotniejszych jest dodanie zdefiniowanego przez nas prompta. Prompt engineering jest sztuką, która potrafi znacząco wpłynąć na działanie RAG. Niekiedy prompty są baaaardzo długie, aby skutecznie nakierować LLM na to o co deweloperowi chodzi. Poniżej, poprosimy o to żeby odpowiedzi były zwięzłe i żeby zawsze kończyły się frazą \"Thanks for asking!\". Ponadto poprosimy o zwracanie dokumentów kontekstowych fraz z odpowiedzią."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2f665-e213-4e2c-9bfa-0294c164bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e44d0-592f-4200-9172-71fa350750de",
   "metadata": {},
   "source": [
    "**Zad. 8: Ponownie zadaj to samo zapytanie. Jak zmieniła się odpowiedź? Zobacz z jakich książek i stron pochodzi kontekst.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c909170-3a9c-4edd-909b-e90f724fe1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf23826-d893-4bac-8722-c0952571ec60",
   "metadata": {},
   "source": [
    "Odpowiedź w istocie jest zwięzła. Poprośmy żeby ją rozwinął."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f25ac3-52c2-42f8-afd7-129b3f40fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"Can you provide a longer response to my last question?\"})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fddb0d4-ed94-4414-ad72-d13e511d49d8",
   "metadata": {},
   "source": [
    "Ups. To chyba nie na temat. Sprawdźmy skąd pochodzi kontekst tej odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93424fe-d03f-490e-8f8f-d85e73d1a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"source_documents\"][0].metadata)\n",
    "print(result[\"source_documents\"][1].metadata)\n",
    "print(result[\"source_documents\"][2].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86ebe7-c4f7-4ee3-b5f6-ffbd51c7535d",
   "metadata": {},
   "source": [
    "Tym razem z bazy wyciągnęliśmy fragmenty wywiadu u Lexa Friedmana. Wynika to stąd, że w obecnej formie nasz RAG nie ma pamięci. Każde zapytanie jest niezależne i nie możemy prowadzić dyskusji pogłębiającej poprzednie pytania. Zaraz to naprawimy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0a2e6-7741-47de-9d10-dcc4818c2908",
   "metadata": {},
   "source": [
    "## Pamięć i czat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02544e-000e-4894-92f3-2f08309eb350",
   "metadata": {},
   "source": [
    "Jeśli zależy nam na dłuższych, wieloetpaowych rozmowach z LLMem, będziemy potrzebować pamięci. Pamięc będzie po prostu zapamiętywać zadane pytania i uzyskane odpowiedzi. Poniżej kod tworzący pamięć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bf6ce-1eb2-4653-8f2a-613160e26612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "memory.clear() # gdy chcemy zresetować pamięć"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18473c97-b6bd-47e5-9c76-d76f1b3f1cd7",
   "metadata": {},
   "source": [
    "Teraz zmienimy łańcuch wywołań na taki korzystający z pamięci. Uwaga, ten chain ma trochę inne nazwy pól zapytania i odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14391427-787c-4a69-8146-923a51e691b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc0b7e-58a3-4926-acba-e557a5730797",
   "metadata": {},
   "source": [
    "Teraz ponówmy nasz eksperyment. Zadajmy pytanie i poprośmy o rozszerzenie poprzedniej odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78038c-9278-4f77-8038-ad75b1003822",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is an eyeball dataset?\"\n",
    "\n",
    "result = chat_chain({\"question\": question})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29713dbc-7250-4b03-8023-e769b8a37177",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat_chain({\"question\": \"Can you provide a longer response to my last question?\"})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116bec4-00db-4d68-a6ca-913bcbaf724c",
   "metadata": {},
   "source": [
    "## Ocena systemu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea084b5-767f-4c3c-b16a-bd7a9a949ad3",
   "metadata": {},
   "source": [
    "Jako ostatni element zapoznamy się z metodami oceny systemów typu RAG. Twórcę takiego systemu może interesować na ile trafne są odpowiedzi, na ile kontekst wspiera odpowiedź i na ile kontekst pasuje do pytania. Te trzy elementy sprawdzają miary Answer Relevance, Groundedness i Context Relevance. Poniżej kod tworzący nowy chain (taki kóry pozwoli zajrzeć do kontekstu i sklei nam wszystkie fragmentu kontekstu w jeden łańcuch znaków. Następnie definicja wspomnienych trzech miar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe73f10-bc6c-42ca-9a78-33b6bfa1ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruChain, Feedback, Huggingface, Tru\n",
    "from trulens_eval.schema import FeedbackResult\n",
    "from trulens_eval.feedback.provider import OpenAI\n",
    "from trulens_eval.app import App\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "import numpy as np\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe916b1-a3d1-4dc4-bb75-689783c26dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | QA_CHAIN_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "openai = OpenAI()\n",
    "context = App.select_context(rag_chain)\n",
    "\n",
    "\n",
    "# Groundedness\n",
    "grounded = Groundedness(groundedness_provider=OpenAI())\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "# Answer Relevance\n",
    "f_qa_relevance = (\n",
    "    Feedback(openai.relevance, name=\"Answer Relevance\").on_input_output()\n",
    ")\n",
    "# Context Relevance\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.qs_relevance, name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68442d2a-6517-4346-9326-cebc0b37b74a",
   "metadata": {},
   "source": [
    "Teraz stworzymy obiekt `tru_recorder`, który będzie monitorował wszystkie zapytania i je oceniał. Zamiast testować na jednym zapytaniu przeprowadzimy eksperyment i policzymy średnią z 7 zapytań żeby ocenić nasz system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4d7a0-8f97-4f24-818e-ddba25910124",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruChain(rag_chain, app_id='ChatApplication', feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])\n",
    "\n",
    "eval_questions = [\n",
    "    'What is an eyeball dataset?',\n",
    "    'What are the keys to building a career in AI?',\n",
    "    'What is the importance of networking in AI?',\n",
    "    'How can altruism be beneficial in building a career?',\n",
    "    'What is imposter syndrome and how does it relate to AI?',\n",
    "    'What will be the impact of AGI on the world?',\n",
    "    'What is the first step to becoming good at AI?',\n",
    "]\n",
    "\n",
    "# To może trochę potrwać...\n",
    "for question in eval_questions:\n",
    "    print(question)\n",
    "    with tru_recorder as recording:\n",
    "        rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d21186-1b34-4ad9-ba5f-1906149fca5a",
   "metadata": {},
   "source": [
    "Możemy zobaczyć jak nasz system sobie radzi uruchamiając komendę `tru.get_leaderboard()`. Jeśli będziemy testować wiele wersji aplikacji (parametr `app_id`) to możemy porównywać wersje między sobą właśnie w ramach tabeli wyników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f3e8e-7a52-43ee-ad7c-3ecb53ea7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589676af-c40d-45a0-9db1-22cb7c08cba0",
   "metadata": {},
   "source": [
    "Na deser: poniższy kod uruchamia dashboard gdzie można przeanalizować każde zapytanie i uzyskane miary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc352a-cdc6-437e-bed9-48bb075ee0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a925fd1-7ca7-4fe2-bf3f-4bf2c484fb4d",
   "metadata": {},
   "source": [
    "**Zad. 9: Czy jesteś w stanie stworzyć nową wersję RAG, która uzyska lepsze metryki? Spróbuj zmienić prompt, żeby odpowiedzi od LLMa były dłuższe. Spróbuj zmienić liczbę fragmentów wydobywanych z bazy. Spróbuj zmienić długość fragmentów, na które dzielone są dokumenty.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47519e-487a-4735-aef7-de17abe9475a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
